Python 3.9.6 (default, Feb  3 2024, 15:58:28)
Type 'copyright', 'credits' or 'license' for more information
IPython 8.18.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import torch

In [3]: import transformers as ppb
/Users/raymbp/Desktop/RayDocs/Projects/SoftwareDev/venv_general_python396/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(

In [4]: model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.Distil
   ...: BertTokenizer, 'distilbert-base-uncased')

In [5]: tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
/Users/raymbp/Desktop/RayDocs/Projects/SoftwareDev/venv_general_python396/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

In [6]: model = model_class.from_pretrained(pretrained_weights)
/Users/raymbp/Desktop/RayDocs/Projects/SoftwareDev/venv_general_python396/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

In [7]: model
Out[7]:
DistilBertModel(
  (embeddings): Embeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer): Transformer(
    (layer): ModuleList(
      (0-5): 6 x TransformerBlock(
        (attention): MultiHeadSelfAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (q_lin): Linear(in_features=768, out_features=768, bias=True)
          (k_lin): Linear(in_features=768, out_features=768, bias=True)
          (v_lin): Linear(in_features=768, out_features=768, bias=True)
          (out_lin): Linear(in_features=768, out_features=768, bias=True)
        )
        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (ffn): FFN(
          (dropout): Dropout(p=0.1, inplace=False)
          (lin1): Linear(in_features=768, out_features=3072, bias=True)
          (lin2): Linear(in_features=3072, out_features=768, bias=True)
          (activation): GELUActivation()
        )
        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
    )
  )
)

In [8]: dir(model)
Out[8]:
['T_destination',
 '__annotations__',
 '__call__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattr__',
 '__getattribute__',
 '__getstate__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__setstate__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_apply',
 '_assisted_decoding',
 '_auto_class',
 '_autoset_attn_implementation',
 '_backward_compatibility_gradient_checkpointing',
 '_backward_hooks',
 '_backward_pre_hooks',
 '_beam_sample',
 '_beam_search',
 '_buffers',
 '_call_impl',
 '_check_and_enable_flash_attn_2',
 '_check_and_enable_sdpa',
 '_compiled_call_impl',
 '_constrained_beam_search',
 '_contrastive_search',
 '_convert_head_mask_to_5d',
 '_copy_lm_head_original_to_resized',
 '_create_repo',
 '_dispatch_accelerate_model',
 '_expand_inputs_for_generation',
 '_extract_past_from_model_output',
 '_forward_hooks',
 '_forward_hooks_always_called',
 '_forward_hooks_with_kwargs',
 '_forward_pre_hooks',
 '_forward_pre_hooks_with_kwargs',
 '_from_config',
 '_get_backward_hooks',
 '_get_backward_pre_hooks',
 '_get_candidate_generator',
 '_get_decoder_start_token_id',
 '_get_files_timestamps',
 '_get_logits_processor',
 '_get_logits_warper',
 '_get_name',
 '_get_no_split_modules',
 '_get_resized_embeddings',
 '_get_resized_lm_head',
 '_get_stopping_criteria',
 '_greedy_search',
 '_group_beam_search',
 '_has_unfinished_sequences',
 '_hf_peft_config_loaded',
 '_hook_rss_memory_post_forward',
 '_hook_rss_memory_pre_forward',
 '_init_weights',
 '_initialize_weights',
 '_is_full_backward_hook',
 '_is_hf_initialized',
 '_is_quantized_training_enabled',
 '_keep_in_fp32_modules',
 '_keep_in_fp32_modules',
 '_keys_to_ignore_on_load_missing',
 '_keys_to_ignore_on_load_unexpected',
 '_keys_to_ignore_on_save',
 '_load_from_state_dict',
 '_load_pretrained_model',
 '_load_pretrained_model_low_mem',
 '_load_state_dict_post_hooks',
 '_load_state_dict_pre_hooks',
 '_maybe_initialize_input_ids_for_generation',
 '_maybe_warn_non_full_backward_hook',
 '_merge_criteria_processor_list',
 '_modules',
 '_named_members',
 '_no_split_modules',
 '_non_persistent_buffers_set',
 '_parameters',
 '_prepare_attention_mask_for_generation',
 '_prepare_decoder_input_ids_for_generation',
 '_prepare_encoder_decoder_kwargs_for_generation',
 '_prepare_generated_length',
 '_prepare_generation_config',
 '_prepare_model_inputs',
 '_prune_heads',
 '_register_load_state_dict_pre_hook',
 '_register_state_dict_hook',
 '_reorder_cache',
 '_replicate_for_data_parallel',
 '_resize_token_embeddings',
 '_sample',
 '_save_to_state_dict',
 '_set_default_torch_dtype',
 '_set_gradient_checkpointing',
 '_skip_keys_device_placement',
 '_slow_forward',
 '_state_dict_hooks',
 '_state_dict_pre_hooks',
 '_supports_cache_class',
 '_supports_flash_attn_2',
 '_supports_sdpa',
 '_temporary_reorder_cache',
 '_tie_encoder_decoder_weights',
 '_tie_or_clone_weights',
 '_tied_weights_keys',
 '_update_model_kwargs_for_generation',
 '_upload_modified_files',
 '_use_flash_attention_2',
 '_validate_generated_length',
 '_validate_model_class',
 '_validate_model_kwargs',
 '_version',
 '_wrapped_call_impl',
 'active_adapter',
 'active_adapters',
 'add_adapter',
 'add_memory_hooks',
 'add_model_tags',
 'add_module',
 'apply',
 'assisted_decoding',
 'base_model',
 'base_model_prefix',
 'beam_sample',
 'beam_search',
 'bfloat16',
 'buffers',
 'call_super_init',
 'can_generate',
 'children',
 'compile',
 'compute_transition_scores',
 'config',
 'config_class',
 'constrained_beam_search',
 'contrastive_search',
 'cpu',
 'create_extended_attention_mask_for_decoder',
 'cuda',
 'device',
 'disable_adapters',
 'disable_input_require_grads',
 'double',
 'dtype',
 'dummy_inputs',
 'dump_patches',
 'embeddings',
 'enable_adapters',
 'enable_input_require_grads',
 'estimate_tokens',
 'eval',
 'extra_repr',
 'float',
 'floating_point_ops',
 'forward',
 'framework',
 'from_pretrained',
 'generate',
 'generation_config',
 'get_adapter_state_dict',
 'get_buffer',
 'get_extended_attention_mask',
 'get_extra_state',
 'get_head_mask',
 'get_input_embeddings',
 'get_memory_footprint',
 'get_output_embeddings',
 'get_parameter',
 'get_position_embeddings',
 'get_submodule',
 'gradient_checkpointing_disable',
 'gradient_checkpointing_enable',
 'greedy_search',
 'group_beam_search',
 'half',
 'init_weights',
 'invert_attention_mask',
 'ipu',
 'is_gradient_checkpointing',
 'is_parallelizable',
 'load_adapter',
 'load_state_dict',
 'load_tf_weights',
 'main_input_name',
 'model_tags',
 'modules',
 'name_or_path',
 'named_buffers',
 'named_children',
 'named_modules',
 'named_parameters',
 'num_parameters',
 'parameters',
 'post_init',
 'prepare_inputs_for_generation',
 'prune_heads',
 'push_to_hub',
 'register_backward_hook',
 'register_buffer',
 'register_for_auto_class',
 'register_forward_hook',
 'register_forward_pre_hook',
 'register_full_backward_hook',
 'register_full_backward_pre_hook',
 'register_load_state_dict_post_hook',
 'register_module',
 'register_parameter',
 'register_state_dict_pre_hook',
 'requires_grad_',
 'reset_memory_hooks_state',
 'resize_position_embeddings',
 'resize_token_embeddings',
 'retrieve_modules_from_names',
 'reverse_bettertransformer',
 'sample',
 'save_pretrained',
 'set_adapter',
 'set_extra_state',
 'set_input_embeddings',
 'share_memory',
 'state_dict',
 'supports_gradient_checkpointing',
 'tie_weights',
 'to',
 'to_bettertransformer',
 'to_empty',
 'train',
 'training',
 'transformer',
 'type',
 'warn_if_padding_and_no_attention_mask',
 'warnings_issued',
 'xpu',
 'zero_grad']

In [9]: model.embeddings
Out[9]:
Embeddings(
  (word_embeddings): Embedding(30522, 768, padding_idx=0)
  (position_embeddings): Embedding(512, 768)
  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)

In [10]: dir(model.embeddings)
Out[10]:
['LayerNorm',
 'T_destination',
 '__annotations__',
 '__call__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattr__',
 '__getattribute__',
 '__getstate__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__setstate__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_apply',
 '_backward_hooks',
 '_backward_pre_hooks',
 '_buffers',
 '_call_impl',
 '_compiled_call_impl',
 '_forward_hooks',
 '_forward_hooks_always_called',
 '_forward_hooks_with_kwargs',
 '_forward_pre_hooks',
 '_forward_pre_hooks_with_kwargs',
 '_get_backward_hooks',
 '_get_backward_pre_hooks',
 '_get_name',
 '_is_full_backward_hook',
 '_is_hf_initialized',
 '_load_from_state_dict',
 '_load_state_dict_post_hooks',
 '_load_state_dict_pre_hooks',
 '_maybe_warn_non_full_backward_hook',
 '_modules',
 '_named_members',
 '_non_persistent_buffers_set',
 '_parameters',
 '_register_load_state_dict_pre_hook',
 '_register_state_dict_hook',
 '_replicate_for_data_parallel',
 '_save_to_state_dict',
 '_slow_forward',
 '_state_dict_hooks',
 '_state_dict_pre_hooks',
 '_version',
 '_wrapped_call_impl',
 'add_module',
 'apply',
 'bfloat16',
 'buffers',
 'call_super_init',
 'children',
 'compile',
 'cpu',
 'cuda',
 'double',
 'dropout',
 'dump_patches',
 'eval',
 'extra_repr',
 'float',
 'forward',
 'get_buffer',
 'get_extra_state',
 'get_parameter',
 'get_submodule',
 'half',
 'ipu',
 'load_state_dict',
 'modules',
 'named_buffers',
 'named_children',
 'named_modules',
 'named_parameters',
 'parameters',
 'position_embeddings',
 'position_ids',
 'register_backward_hook',
 'register_buffer',
 'register_forward_hook',
 'register_forward_pre_hook',
 'register_full_backward_hook',
 'register_full_backward_pre_hook',
 'register_load_state_dict_post_hook',
 'register_module',
 'register_parameter',
 'register_state_dict_pre_hook',
 'requires_grad_',
 'set_extra_state',
 'share_memory',
 'state_dict',
 'to',
 'to_empty',
 'train',
 'training',
 'type',
 'word_embeddings',
 'xpu',
 'zero_grad']

In [11]: input_ids=torch.tensor([[101, 23, 45, 23, 102], [101, 25, 34, 22, 102]])

In [12]: model.embeddings.word_embeddings
Out[12]: Embedding(30522, 768, padding_idx=0)
